## scrapy_redis与scrapy的对比

##### 学习目标：
1. 了解 scrapy_redis和scrapy在概念上的区别 
2. 了解 scrapy_redis的运行流程
3. 了解 scrapy_redis对request指纹实现的原理
4. 了解 scrapy_redis爬虫settings.py文件的配置
5. 掌握 口述scarpy_redis实现增量式爬虫、布式爬虫、crwalspider爬虫
----


### 1. scrapy_redis的含义和能够实现的功能
1. scrapy是框架
2. scrapy_redis是scrapy的组件
3. scrapy_redis能够实现断点续爬和分布式爬虫

### 2. scrapy_redis流程和实现原理
1. 在scrapy框架流程的基础上，把存储request对象的队列放到了redis的有序集合中，利用该有序集合实现了分布式
2. 并对request对象生成指纹对象，也存储到redis，利用request指纹避免发送重复的请求，实现了断点续爬
3. 每次都从redis中拿取request对象并比对指纹库后，进行去重，然后才发送没有发送过的请求
4. RedisSpider和RedisCrawlSpider两个爬虫类默认把获取的数据存储到redis当中
5. request指纹的实现
    - 请求方法
    - 排序后的请求地址
    - 请求体或空字符串
    - 用hashlib.sha1()对以上内容进行加密

### 3. scarpy_redis实现增量式爬虫、布式爬虫、crwalspider爬虫
1. 对setting进行如下设置

```
DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"
SCHEDULER = "scrapy_redis.scheduler.Scheduler"
SCHEDULER_PERSIST = True
ITEM_PIPELINES = {'scrapy_redis.pipelines.RedisPipeline': 400,}
REDIS_URL = "redis://127.0.0.1:6379"
```

2. 在爬虫文件中爬虫继承RedisSpider类，或RedisCrawlSpider类
3. 在爬虫文件中redis_key替代了start_urls
4. 手动指定allowed_domain来代替__init__函数
5. RedisCrawlSpider类完善rules规则，并且不能使用parse()方法
6. 不管是不是分布式爬虫，都要正确配置REDIS_URL
7. 通过`scrapy crawl spider`启动爬虫后，向redis_key放入一个或多个起始url（lpush或rpush都可以），才能启动scrapy_redis爬虫
8. 除了以上差异点以外，scrapy_redis爬虫和scrapy爬虫的使用方法都是一样的


----

### 总结
1. scrapy_redis和scrapy在概念上的区别 
2. scrapy_redis的运行流程
3. scrapy_redis对request指纹实现的原理
4. scrapy_redis爬虫settings.py文件的配置
5. scarpy_redis实现增量式爬虫、布式爬虫、crwalspider爬虫